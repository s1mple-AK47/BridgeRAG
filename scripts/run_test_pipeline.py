"""
ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬ï¼šå¤„ç† HotpotQA æ•°æ®é›†çš„å‰ 20 æ¡æ•°æ®ï¼Œèµ°å®Œæ•´ä¸ªç¦»çº¿+åœ¨çº¿æµç¨‹ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç¡®ä¿ Docker æœåŠ¡å·²å¯åŠ¨: docker-compose up -d
2. ç¡®ä¿ vLLM æœåŠ¡å·²å¯åŠ¨
3. åœ¨ä¸€ä¸ªç»ˆç«¯å¯åŠ¨ Celery Worker:
   celery -A bridgerag.celery_app worker --loglevel=info -c 4 -Q offline_processing
4. åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œæ­¤è„šæœ¬:
   python scripts/run_test_pipeline.py
"""

import json
import logging
import sys
import time
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from bridgerag.utils.logging_config import setup_logging
from bridgerag.config import settings

setup_logging()
logger = logging.getLogger(__name__)

# ============== é…ç½® ==============
TEST_DOC_LIMIT = 20  # æµ‹è¯•æ–‡æ¡£æ•°é‡
TEST_QUESTION_LIMIT = 5  # æµ‹è¯•é—®é¢˜æ•°é‡
# ==================================


def step1_convert_data():
    """æ­¥éª¤1ï¼šè½¬æ¢æ•°æ®æ ¼å¼ï¼ˆåªå–å‰ N æ¡ï¼‰"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 1: è½¬æ¢æ•°æ®æ ¼å¼")
    logger.info("=" * 60)
    
    wiki_input = project_root / "datas" / "wiki_pages_final.jsonl"
    qa_input = project_root / "datas" / "hotpot_qa_filtered.jsonl"
    
    docs_output = project_root / "test_docs.jsonl"
    questions_output = project_root / "test_questions.jsonl"
    
    if not wiki_input.exists():
        logger.error(f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {wiki_input}")
        return False
    
    if not qa_input.exists():
        logger.error(f"é—®ç­”æ–‡ä»¶ä¸å­˜åœ¨: {qa_input}")
        return False
    
    # è½¬æ¢æ–‡æ¡£ï¼ˆå–å‰ N æ¡ï¼‰
    doc_count = 0
    seen_ids = set()
    with open(wiki_input, 'r', encoding='utf-8') as infile, \
         open(docs_output, 'w', encoding='utf-8') as outfile:
        for line in infile:
            if doc_count >= TEST_DOC_LIMIT:
                break
            data = json.loads(line.strip())
            doc_id = data.get("title", "").strip()
            content = data.get("content", "").strip()
            if doc_id and content and doc_id not in seen_ids:
                seen_ids.add(doc_id)
                outfile.write(json.dumps({"id": doc_id, "text": content}, ensure_ascii=False) + '\n')
                doc_count += 1
    
    logger.info(f"å·²è½¬æ¢ {doc_count} ç¯‡æ–‡æ¡£ -> {docs_output}")
    
    # è½¬æ¢é—®ç­”ï¼ˆå–å‰ N æ¡ï¼‰
    question_count = 0
    with open(qa_input, 'r', encoding='utf-8') as infile, \
         open(questions_output, 'w', encoding='utf-8') as outfile:
        for i, line in enumerate(infile, 1):
            if question_count >= TEST_QUESTION_LIMIT:
                break
            data = json.loads(line.strip())
            question = data.get("question", "").strip()
            answer = data.get("answer", "").strip()
            titles = data.get("title", [])
            if question and answer:
                unique_titles = list(dict.fromkeys(titles))
                outfile.write(json.dumps({
                    "id": f"q_{i}",
                    "question": question,
                    "answer": answer,
                    "ids": unique_titles
                }, ensure_ascii=False) + '\n')
                question_count += 1
    
    logger.info(f"å·²è½¬æ¢ {question_count} ä¸ªé—®é¢˜ -> {questions_output}")
    return True


def step2_initialize_databases():
    """æ­¥éª¤2ï¼šåˆå§‹åŒ–æ•°æ®åº“"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 2: åˆå§‹åŒ–æ•°æ®åº“")
    logger.info("=" * 60)
    
    from urllib.parse import urlparse
    from bridgerag.database.vector_db import VectorDBConnection
    from bridgerag.database import vector_ops
    
    try:
        parsed_milvus_uri = urlparse(settings.milvus_uri)
        vector_db_conn = VectorDBConnection(
            host=parsed_milvus_uri.hostname,
            port=parsed_milvus_uri.port,
            alias="default"
        )
        
        # åˆ›å»ºé›†åˆ
        vector_ops.create_chunk_collection(
            collection_name=settings.chunk_collection_name,
            dense_dim=settings.embedding_dim,
            text_max_length=settings.chunk_max_length
        )
        vector_ops.create_entity_collection(
            collection_name=settings.entity_collection_name,
            dense_dim=settings.embedding_dim,
            text_max_length=settings.entity_summary_max_length
        )
        vector_ops.create_summary_collection(
            collection_name=settings.summary_collection_name,
            dense_dim=settings.embedding_dim,
            text_max_length=settings.summary_max_length
        )
        
        vector_db_conn.close()
        logger.info("Milvus é›†åˆåˆå§‹åŒ–å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
        return False


def step3_run_offline_pipeline():
    """æ­¥éª¤3ï¼šè¿è¡Œç¦»çº¿æµæ°´çº¿ï¼ˆé€šè¿‡ Celeryï¼‰"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 3: è¿è¡Œç¦»çº¿æµæ°´çº¿")
    logger.info("=" * 60)
    
    from bridgerag.offline.pipeline import trigger_batch_processing
    from bridgerag.database.object_storage import ObjectStorageConnection
    import bridgerag.database.object_storage_ops as storage_ops
    
    data_file = project_root / "test_docs.jsonl"
    
    if not data_file.exists():
        logger.error(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return False
    
    # èŽ·å–å·²å¤„ç†çš„æ–‡æ¡£
    processed_ids = set()
    try:
        minio_conn = ObjectStorageConnection(
            endpoint=settings.minio_endpoint,
            access_key=settings.minio_access_key,
            secret_key=settings.minio_secret_key
        )
        if minio_conn.client.bucket_exists(settings.minio_bucket_name):
            object_names = storage_ops.list_objects(minio_conn.client, settings.minio_bucket_name)
            processed_ids = {Path(obj_name).stem for obj_name in object_names}
        logger.info(f"å·²å¤„ç†æ–‡æ¡£æ•°: {len(processed_ids)}")
    except Exception as e:
        logger.warning(f"æ£€æŸ¥ MinIO æ—¶å‡ºé”™: {e}")
    
    # åŠ è½½æ–‡æ¡£
    documents = []
    with open(data_file, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            if data["id"] not in processed_ids:
                documents.append(data)
    
    if not documents:
        logger.info("æ‰€æœ‰æ–‡æ¡£å·²å¤„ç†å®Œæ¯•")
        return True
    
    logger.info(f"å¾…å¤„ç†æ–‡æ¡£æ•°: {len(documents)}")
    
    # æäº¤ä»»åŠ¡
    batch_for_pipeline = [(doc["id"], {"text": doc["text"]}) for doc in documents]
    
    logger.info("æ­£åœ¨æäº¤ä»»åŠ¡åˆ° Celery...")
    task_group_result = trigger_batch_processing(batch_for_pipeline)
    
    if task_group_result:
        logger.info(f"ä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ç»„ ID: {task_group_result.id}")
        logger.info("ç­‰å¾…ä»»åŠ¡å®Œæˆï¼ˆè¶…æ—¶æ—¶é—´: 30 åˆ†é’Ÿï¼‰...")
        try:
            task_group_result.get(timeout=1800)  # 30 åˆ†é’Ÿè¶…æ—¶
            logger.info("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
            return True
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            return False
    else:
        logger.error("ä»»åŠ¡æäº¤å¤±è´¥")
        return False


def step4_run_entity_linking():
    """æ­¥éª¤4ï¼šè¿è¡Œå®žä½“é“¾æŽ¥"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 4: è¿è¡Œå®žä½“é“¾æŽ¥")
    logger.info("=" * 60)
    
    from urllib.parse import urlparse
    from bridgerag.database.graph_db import GraphDBConnection
    from bridgerag.database.vector_db import VectorDBConnection
    from bridgerag.core.llm_client import LLMClient
    from bridgerag.offline.steps.link_entities import run_entity_linking
    
    try:
        llm_client = LLMClient()
        gdb_conn = GraphDBConnection(
            uri=settings.neo4j_uri,
            user=settings.neo4j_user,
            password=settings.neo4j_password
        )
        
        parsed_milvus_uri = urlparse(settings.milvus_uri)
        VectorDBConnection(host=parsed_milvus_uri.hostname, port=parsed_milvus_uri.port)
        
        run_entity_linking(
            driver=gdb_conn._driver,
            llm_client=llm_client,
            milvus_collection_name=settings.entity_collection_name,
            max_workers=4
        )
        
        gdb_conn.close()
        logger.info("å®žä½“é“¾æŽ¥å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"å®žä½“é“¾æŽ¥å¤±è´¥: {e}", exc_info=True)
        return False


def step5_run_online_query():
    """æ­¥éª¤5ï¼šè¿è¡Œåœ¨çº¿æŸ¥è¯¢æµ‹è¯•"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 5: è¿è¡Œåœ¨çº¿æŸ¥è¯¢æµ‹è¯•")
    logger.info("=" * 60)
    
    from bridgerag.online.main import OnlineQueryProcessor
    
    questions_file = project_root / "test_questions.jsonl"
    
    if not questions_file.exists():
        logger.error(f"æµ‹è¯•é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {questions_file}")
        return False
    
    # åŠ è½½é—®é¢˜
    questions = []
    with open(questions_file, 'r', encoding='utf-8') as f:
        for line in f:
            questions.append(json.loads(line.strip()))
    
    logger.info(f"åŠ è½½äº† {len(questions)} ä¸ªæµ‹è¯•é—®é¢˜")
    
    try:
        processor = OnlineQueryProcessor()
        
        results = []
        for i, q in enumerate(questions, 1):
            logger.info(f"\n--- é—®é¢˜ {i}/{len(questions)} ---")
            logger.info(f"é—®é¢˜: {q['question']}")
            logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {q['answer']}")
            
            start_time = time.time()
            result = processor.process_query(q['question'])
            duration = time.time() - start_time
            
            logger.info(f"LLM ç­”æ¡ˆ: {result.answer}")
            logger.info(f"ç›¸å…³æ–‡æ¡£: {result.main_documents}")
            logger.info(f"è€—æ—¶: {duration:.2f}s")
            
            results.append({
                "id": q["id"],
                "question": q["question"],
                "answer": q["answer"],
                "LLM_answer": result.answer,
                "LLM_docs": result.main_documents,
                "duration": round(duration, 2)
            })
        
        processor.close()
        
        # ä¿å­˜ç»“æžœ
        output_file = project_root / "test_results.jsonl"
        with open(output_file, 'w', encoding='utf-8') as f:
            for r in results:
                f.write(json.dumps(r, ensure_ascii=False) + '\n')
        
        logger.info(f"\nç»“æžœå·²ä¿å­˜åˆ°: {output_file}")
        return True
        
    except Exception as e:
        logger.error(f"åœ¨çº¿æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
        return False


def main():
    """ä¸»å‡½æ•°ï¼šæŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰æ­¥éª¤"""
    logger.info("=" * 60)
    logger.info("BridgeRAG ç«¯åˆ°ç«¯æµ‹è¯•")
    logger.info(f"æµ‹è¯•æ–‡æ¡£æ•°: {TEST_DOC_LIMIT}, æµ‹è¯•é—®é¢˜æ•°: {TEST_QUESTION_LIMIT}")
    logger.info("=" * 60)
    
    steps = [
        ("è½¬æ¢æ•°æ®", step1_convert_data),
        ("åˆå§‹åŒ–æ•°æ®åº“", step2_initialize_databases),
        ("ç¦»çº¿æµæ°´çº¿", step3_run_offline_pipeline),
        ("å®žä½“é“¾æŽ¥", step4_run_entity_linking),
        ("åœ¨çº¿æŸ¥è¯¢", step5_run_online_query),
    ]
    
    for step_name, step_func in steps:
        logger.info(f"\n>>> å¼€å§‹æ‰§è¡Œ: {step_name}")
        success = step_func()
        if not success:
            logger.error(f"<<< {step_name} å¤±è´¥ï¼Œæµ‹è¯•ä¸­æ­¢")
            return
        logger.info(f"<<< {step_name} å®Œæˆ")
    
    logger.info("\n" + "=" * 60)
    logger.info("ðŸŽ‰ ç«¯åˆ°ç«¯æµ‹è¯•å…¨éƒ¨å®Œæˆï¼")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
